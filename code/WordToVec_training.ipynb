{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pyprind\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# used when validating\n",
    "train_pos=pd.read_csv(\"Y://Data//rev_Imdb//train_pos.csv\",\n",
    "                      encoding=\"cp1252\")\n",
    "\n",
    "train_neg=pd.read_csv(\"Y://Data//rev_Imdb//train_neg.csv\",\n",
    "                      encoding=\"cp1252\")\n",
    "train_data=pd.concat([train_pos,train_neg]).loc[:,['Review','Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "def tokenizer_stem_porter(text):\n",
    "    porter=PorterStemmer()\n",
    "    stop=stopwords.words('english')\n",
    "    textlst=[porter.stem(word) for word in text.split() \n",
    "             if word not in stop]\n",
    "    return ' '.join(textlst)\n",
    "\n",
    "\n",
    "def tokenizer_stem_wordnet(text):\n",
    "    #porter=PorterStemmer()\n",
    "    lmtzr=WordNetLemmatizer()\n",
    "    stop=stopwords.words('english')\n",
    "    textlst=[lmtzr.lemmatize(word) for word in text.split() \n",
    "             if word not in stop]\n",
    "    return ' '.join(textlst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordtovec(Input_data):\n",
    "    \n",
    "    doc_lst=Input_data.Review.apply(tokenizer_stem_wordnet)\n",
    "    print(\"#######################stem finished######################\")\n",
    "    X_data=pd.DataFrame(columns=['Review'])\n",
    "    X_data['Review']=doc_lst\n",
    "    Y_data=Input_data.Sentiment\n",
    "    \n",
    "    tfidf = TfidfVectorizer(min_df=1)\n",
    "    X_data=tfidf.fit_transform(X_data.Review.tolist())\n",
    "    print(\"###################### data ready #########################\")\n",
    "    \n",
    "    return X_data,Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    train_pos=pd.read_csv(\"Y://Data//rev_Imdb//train_pos.csv\",\n",
    "                          encoding=\"cp1252\")\n",
    "    train_neg=pd.read_csv(\"Y://Data//rev_Imdb//train_neg.csv\",\n",
    "                          encoding=\"cp1252\")\n",
    "    train_data=pd.concat([train_pos,train_neg]).loc[:,\n",
    "                               ['Review','Sentiment']]\n",
    "    test_pos=pd.read_csv(\"Y://Data//rev_Imdb//test_pos.csv\",\n",
    "                         encoding=\"cp1252\")\n",
    "    test_neg=pd.read_csv(\"Y://Data//rev_Imdb//test_neg.csv\",\n",
    "                         encoding=\"cp1252\")\n",
    "    test_data=pd.concat([test_pos,test_neg]).loc[:,['Review','Sentiment']]\n",
    "    \n",
    "    nrows=train_data.shape[0]\n",
    "    \n",
    "    all_data=pd.concat([train_data,test_data])\n",
    "    X_vec, Y_vec=wordtovec(all_data)\n",
    "    \n",
    "    X_train=X_vec[:nrows,:]\n",
    "    Y_train=Y_vec[:nrows]\n",
    "    X_test =X_vec[nrows:,:]\n",
    "    Y_test=Y_vec[nrows:]\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_svm(X, y):\n",
    "    \"\"\"\n",
    "    Create and train the Support Vector Machine.\n",
    "    \"\"\"\n",
    "    svm = SVC(C=1000.0, gamma=0.0001, kernel='rbf')\n",
    "    svm.fit(X, y)\n",
    "    \n",
    "    return svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_logistic(X, y):\n",
    "    \"\"\"\n",
    "    Create and train the Logistic regression model.\n",
    "    \"\"\"\n",
    "    logis = LogisticRegression(C=10.0, verbose=5,\n",
    "                               n_jobs=-1, solver='sag')\n",
    "    logis.fit(X, y)\n",
    "    \n",
    "    return logis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_rf(X,y):\n",
    "    rfc= RandomForestClassifier(n_estimators=220, criterion='gini',\n",
    "                                max_depth=19, n_jobs=-1,verbose=1)\n",
    "    rfc.fit(X, y)\n",
    "    \n",
    "    return rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gnb(X,y):\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X,y)\n",
    "    \n",
    "    return gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_bnb(X,y):\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(X,y)\n",
    "    \n",
    "    return bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_mnb(X,y):\n",
    "    bnb = MultinomialNB()\n",
    "    bnb.fit(X,y)\n",
    "    \n",
    "    return bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_knn(X,y):\n",
    "    knn=KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X, y) \n",
    "    \n",
    "    return knn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_rf(X, y):\n",
    "    \n",
    "    clf = RandomForestClassifier(criterion='gini')\n",
    "    tuned_parameters = [{'n_estimators': [210,220,240],\n",
    "                     'max_depth': [17,18,19]}]\n",
    "    search = GridSearchCV(clf, tuned_parameters, cv=5,scoring='accuracy',\n",
    "                          verbose=100,n_jobs=1)\n",
    "    search.fit(X, y)\n",
    "    print(\"best param:\",search.best_params_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grid_svm(X, y):\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    tuned_parameters = [{'kernel': ['rbf','poly'], 'gamma': [0.0001],\n",
    "                     'C': [1000,2000,500]}]\n",
    "    search = GridSearchCV(clf, tuned_parameters, cv=5,scoring='accuracy',\n",
    "                          verbose=10,n_jobs=-1)\n",
    "    search.fit(X, y)\n",
    "    print(\"best param:\",search.best_params_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_knn(X,y):\n",
    "    clf=KNeighborsClassifier()\n",
    "    \n",
    "    tuned_parameters = [{'n_neighbors': [3,5,7], \n",
    "                         'algorithm': ['ball_tree','auto'],\n",
    "                         'p': [1,2]}]\n",
    "    search = GridSearchCV(clf, tuned_parameters, cv=5, scoring='accuracy',\n",
    "                          n_jobs=-1,verbose=3)\n",
    "    search.fit(X, y) \n",
    "    \n",
    "    print(\"best param:\",search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main_cv(train_data):\n",
    "\n",
    "    X_train, Y_train=wordtovec(train_data)\n",
    "\n",
    "    grid_svm(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[283, 2284]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validating results\n",
    "\n",
    "#---------------------- SVM Reg Train ------------------\n",
    "#C=1000, gamma=0.0001\n",
    "0.8854\n",
    "[2144 , 232]\n",
    "[ 341 ,2283]\n",
    "\n",
    "#----------------------Logistic Regression--------------------\n",
    "#'sag' C=10.0  convergence after 31 epochs took 1 seconds\n",
    "0.8972\n",
    "[2202 , 231]\n",
    "[ 283, 2284]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################stem finished######################\n",
      "###################### data ready #########################\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 18.8min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 32.2min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 55.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best param: {'C': 1000, 'kernel': 'rbf', 'gamma': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "main_cv(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main_val():\n",
    "    \n",
    "#def main_svm()\n",
    "\n",
    "    X_train, Y_train=wordtovec(train_data)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X_train, Y_train,\n",
    "                                test_size=0.2, random_state=42)\n",
    "\n",
    "    #svm = train_svm(X_tr, y_tr)\n",
    "    bnb= train_mnb(X_tr.toarray(),y_tr)\n",
    "\n",
    "    # Make an array of predictions on the test set\n",
    "    #pred = svm.predict(X_te)\n",
    "    pred=bnb.predict(X_te.toarray())\n",
    "\n",
    "    # Output the hit-rate and the confusion matrix for each model\n",
    "    print(bnb.score(X_te.toarray(), y_te))\n",
    "    print(confusion_matrix(pred, y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main_val():\n",
    "\n",
    "    X_train, Y_train=wordtovec(train_data)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X_train, Y_train, \n",
    "                              test_size=0.2, random_state=1024)\n",
    "\n",
    "    logis = train_logistic(X_tr, y_tr)\n",
    "\n",
    "    # Make an array of predictions on the test set\n",
    "    pred = logis.predict(X_te)\n",
    "\n",
    "    # Output the hit-rate and the confusion matrix for each model\n",
    "    print(logis.score(X_te, y_te))\n",
    "    print(confusion_matrix(pred, y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_test():\n",
    "\n",
    "    X_train, Y_train,X_test, Y_test=read_data()\n",
    "   \n",
    "    model=train_knn(X_train, Y_train)\n",
    "\n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    # Output the hit-rate and the confusion matrix for each model\n",
    "    print(model.score(X_test, Y_test))\n",
    "    print(confusion_matrix(pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################stem finished########################\n",
      "######################### data ready ############################\n",
      "convergence after 33 epochs took 1 seconds\n",
      "0.87248\n",
      "[[11065  1753]\n",
      " [ 1435 10747]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "#logistic\n",
    "main_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################stem finished########################\n",
      "######################### data ready ############################\n",
      "0.81696\n",
      "[[11102  3178]\n",
      " [ 1398  9322]]\n"
     ]
    }
   ],
   "source": [
    "#bernoulli naive basyes\n",
    "main_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################stem finished########################\n",
      "######################### data ready ############################\n",
      "0.831\n",
      "[[10988  2713]\n",
      " [ 1512  9787]]\n"
     ]
    }
   ],
   "source": [
    "#multi NB\n",
    "main_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################stem finished########################\n",
      "######################### data ready ############################\n",
      "0.65128\n",
      "[[8299 4517]\n",
      " [4201 7983]]\n"
     ]
    }
   ],
   "source": [
    "#knn\n",
    "main_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################stem finished########################\n",
      "######################### data ready ############################\n",
      "0.87904\n",
      "[[10895  1419]\n",
      " [ 1605 11081]]\n"
     ]
    }
   ],
   "source": [
    "#svm\n",
    "main_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################stem finished########################\n",
      "######################### data ready ############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 220 out of 220 | elapsed:   10.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 220 out of 220 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8474\n",
      "[[10344  1659]\n",
      " [ 2156 10841]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 220 out of 220 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "#rf\n",
    "main_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
